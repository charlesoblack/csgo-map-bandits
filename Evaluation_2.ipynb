{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dynamic-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandit import Bandit\n",
    "from context_engineering_functions import *\n",
    "from logging_policy import LoggingPolicy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from evaluation import train_value_estimator, evaluate\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-ivory",
   "metadata": {},
   "source": [
    "Need to take a Bandit object - which contains a policy to evaluate, and also a Logging Policy Object.\n",
    "We estimate the value using the Direct Method, where we fit a ridge regression w/ importance weights onto the reward function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "false-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = './csgo_clean/'\n",
    "map_pick_context, vetoes_only_context = create_basic_pick_veto_triples(data_directory) # Not important, but loaded vetoes too\n",
    "map_pick_context_train, map_pick_context_test  = train_test_split(map_pick_context,test_size=.2,train_size=.8,shuffle=False)\n",
    "\n",
    "cols = [col for col in map_pick_context if col.endswith('is_available')]\n",
    "X_train = map_pick_context_train[cols].values\n",
    "A_train = map_pick_context_train['X_Action'].values\n",
    "Y_train = map_pick_context_train['Y_reward'].values\n",
    "\n",
    "X_test = map_pick_context_test[cols].values\n",
    "A_test = map_pick_context_test['X_Action'].values\n",
    "Y_test = map_pick_context_test['Y_reward'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "informed-sauce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for random row: \n",
      "[[0.2 0.2 0.  0.2 0.2 0.2 0. ]]\n"
     ]
    }
   ],
   "source": [
    "n_arms = 7\n",
    "\n",
    "# randomly chosen by dice roll\n",
    "n = 4569\n",
    "# n = np.random.choice(context.index)\n",
    "\n",
    "bandit = Bandit(X_train.shape[1], n_arms, step_size=0.01)\n",
    "\n",
    "print('Probabilities for random row: ')\n",
    "print(bandit.predict_proba(X_train[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "younger-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = LoggingPolicy(map_pick_context_train,map_pick_context_train['X_Action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "disciplinary-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: multiple epochs, parameter tuning\n",
    "for i in range(X_train.shape[0]):\n",
    "    bandit.update_theta(X_train[i].reshape(1, -1), A_train[i], Y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "disabled-friend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16526486 0.2547316  0.         0.26307424 0.1534473  0.163482\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(bandit.predict_proba(X_train[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-decline",
   "metadata": {},
   "source": [
    "### Multi-epoch loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "challenging-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: multiple epochs, parameter tuning\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(X_train.shape[0]):\n",
    "        bandit.update_theta(X_train[i].reshape(1, -1), A_train[i], Y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "periodic-intention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15566783 0.25676085 0.         0.27968643 0.1492302  0.15865469\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(bandit.predict_proba(X_train[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sweet-friend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for random row after training: \n",
      "[[0.15566783 0.25676085 0.         0.27968643 0.1492302  0.15865469\n",
      "  0.        ]]\n",
      "Selected action: \n",
      "[3]\n",
      "Actual action and reward for random row: \n",
      "Action: 4, reward: 0\n"
     ]
    }
   ],
   "source": [
    "print('Probabilities for random row after training: ')\n",
    "print(bandit.predict_proba(X_train[n]))\n",
    "print('Selected action: ')\n",
    "print(bandit.predict(X_train[n]))\n",
    "\n",
    "print('Actual action and reward for random row: ')\n",
    "print(f'Action: {A_train[n]}, reward: {Y_train[n]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-audio",
   "metadata": {},
   "source": [
    "MHS: You had X_train hardcoded in the function a few times, and I updated your assert in evaluate() because I removed log_policy.Y from LoggingPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "automotive-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_model_dict = train_value_estimator(X_train,map_pick_context_train, A_train, Y_train, log_policy=lp, target_bandit=bandit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fresh-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform policy: untrained bandit\n",
    "untrained_bandit = Bandit(X_train.shape[1], n_arms, step_size=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-kingdom",
   "metadata": {},
   "source": [
    "MHS: Nothing important different down here, except I added the Test data section too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-musician",
   "metadata": {},
   "source": [
    "### Eval on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "armed-aruba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Bandit:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': 0.5495714285714286,\n",
       " 'IW': 1.3000251226930468,\n",
       " 'SN_IW': 0.5582780985246645,\n",
       " 'Direct_Method_IW': 0.5583079306013885}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Trained Bandit:\")\n",
    "evaluate(X_train, map_pick_context_train, A_train, Y_train, \\\n",
    "         log_policy=lp, target_bandit=bandit, action_to_model_dict=action_to_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abroad-lecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform Policy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': 0.5495714285714286,\n",
       " 'IW': 1.254442289045975,\n",
       " 'SN_IW': 0.5558585769907851,\n",
       " 'Direct_Method_IW': 0.5559663867915816}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Uniform Policy:\")\n",
    "evaluate(X_train, map_pick_context_train, A_train, Y_train, \\\n",
    "         log_policy=lp, target_bandit=untrained_bandit,action_to_model_dict=action_to_model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-loading",
   "metadata": {},
   "source": [
    "### Eval on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "american-priest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Bandit:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "18",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ca3e2fc39dab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trained Bandit:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m evaluate(X_test, map_pick_context_test, A_test, Y_test, \\\n\u001b[0m\u001b[1;32m      3\u001b[0m          log_policy=lp, target_bandit=bandit, action_to_model_dict=action_to_model_dict)\n",
      "\u001b[0;32m~/csgo-map-bandits/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(context_test, map_picks_test, actions_test, rewards_test, log_policy, target_bandit, action_to_model_dict)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mlog_propensities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_picks_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mlog_propensities_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mlog_propensities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_propensities_row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/csgo-map-bandits/logging_policy.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, is_veto)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Zero out probability for unavailable maps, normalize the other probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mpa_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpa_x_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DecisionTeamId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 18"
     ]
    }
   ],
   "source": [
    "print(\"Trained Bandit:\")\n",
    "evaluate(X_test, map_pick_context_test, A_test, Y_test, \\\n",
    "         log_policy=lp, target_bandit=bandit, action_to_model_dict=action_to_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Uniform Policy:\")\n",
    "evaluate(X_test, map_pick_context_test, A_test, Y_test, \\\n",
    "         log_policy=lp, target_bandit=untrained_bandit, action_to_model_dict=action_to_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-trouble",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
