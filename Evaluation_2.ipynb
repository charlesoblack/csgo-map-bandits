{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandit import Bandit\n",
    "from context_engineering_functions import *\n",
    "from logging_policy import LoggingPolicy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to take a Bandit object - which contains a policy to evaluate, and also a Logging Policy Object.\n",
    "We estimate the value using the Direct Method, where we fit a ridge regression w/ importance weights onto the reward function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '../data/clean/'\n",
    "map_pick_context, vetoes_only_context = create_basic_pick_veto_triples(data_directory) # Not important, but loaded vetoes too\n",
    "map_pick_context_train, map_pick_context_test  = train_test_split(map_pick_context,test_size=.2,train_size=.8,shuffle=False)\n",
    "\n",
    "cols = [col for col in map_pick_context if col.endswith('is_available')]\n",
    "X_train = map_pick_context_train[cols].values\n",
    "A_train = map_pick_context_train['X_Action'].values\n",
    "Y_train = map_pick_context_train['Y_reward'].values\n",
    "\n",
    "X_test = map_pick_context_test[cols].values\n",
    "A_test = map_pick_context_test['X_Action'].values\n",
    "Y_test = map_pick_context_test['Y_reward'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for random row: \n",
      "[[0.2 0.2 0.  0.2 0.2 0.2 0. ]]\n"
     ]
    }
   ],
   "source": [
    "n_arms = 7\n",
    "\n",
    "# randomly chosen by dice roll\n",
    "n = 4569\n",
    "# n = np.random.choice(context.index)\n",
    "\n",
    "bandit = Bandit(X_train.shape[1], n_arms, step_size=0.01)\n",
    "\n",
    "print('Probabilities for random row: ')\n",
    "print(bandit.predict_proba(X_train[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = LoggingPolicy(map_pick_context_train,map_pick_context_train['X_Action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: multiple epochs, parameter tuning\n",
    "for i in range(X_train.shape[0]):\n",
    "    bandit.update_theta(X_train[i].reshape(1, -1), A_train[i], Y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16526486 0.2547316  0.         0.26307424 0.1534473  0.163482\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(bandit.predict_proba(X_train[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-epoch loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: multiple epochs, parameter tuning\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(X_train.shape[0]):\n",
    "        bandit.update_theta(X_train[i].reshape(1, -1), A_train[i], Y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14596562 0.25995308 0.         0.27543339 0.15278697 0.16586094\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(bandit.predict_proba(X_train[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for random row after training: \n",
      "[[0.16526486 0.2547316  0.         0.26307424 0.1534473  0.163482\n",
      "  0.        ]]\n",
      "Selected action: \n",
      "[3]\n",
      "Actual action and reward for random row: \n",
      "Action: 4, reward: 0\n"
     ]
    }
   ],
   "source": [
    "print('Probabilities for random row after training: ')\n",
    "print(bandit.predict_proba(X_train[n]))\n",
    "print('Selected action: ')\n",
    "print(bandit.predict(X_train[n]))\n",
    "\n",
    "print('Actual action and reward for random row: ')\n",
    "print(f'Action: {A_train[n]}, reward: {Y_train[n]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MHS: You had X_train hardcoded in the function a few times, and I updated your assert in evaluate() because I removed log_policy.Y from LoggingPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_value_estimator(context_train,map_picks_train,actions_train,rewards_train,log_policy,target_bandit):\n",
    "    '''\n",
    "    Trains an importance weighted RidgeCV model which is used for direct method estimation.\n",
    "    \n",
    "    Input:\n",
    "        context_train (np.array(n x k)): n is number of train examples. k is dimensionality of context.\n",
    "        map_picks_train (df)\n",
    "            map_picks train and context train must be created using this line of code:\n",
    "            \"context_train = map_pick_context_train[cols].values\"\n",
    "\n",
    "        actions_train (np.array(n)) : actions taken\n",
    "        rewards (np.array(n): rewards received\n",
    "        log_policy: a LoggingPolicy object, which needs to have a function predict_proba(self,context)\n",
    "        target_policy: a Bandit object, which needs to have a function predict_proba(self,context)\n",
    "    '''\n",
    "    all_actions = np.unique(actions_train) # return from unique is already sorted\n",
    "    action_to_model_dict = {}\n",
    "    log_propensities = np.empty((context_train.shape[0],len(log_policy.map_cols)))\n",
    "    \n",
    "    for ii,(idx,row) in enumerate(map_picks_train.iterrows()):   \n",
    "        log_propensities_row = log_policy.predict_proba(row)\n",
    "        log_propensities[ii,:] = log_propensities_row\n",
    "    \n",
    "    target_propensities = np.empty((context_train.shape[0],len(log_policy.map_cols)))\n",
    "    \n",
    "    for ii in range(context_train.shape[0]):   \n",
    "        target_propensities_row = target_bandit.predict_proba(context_train[ii,:])\n",
    "        target_propensities[ii,:] = target_propensities_row\n",
    "    \n",
    "    # Check to make sure these are both n x k. I guess its possible that not all actions were chosen, but thats unlikely.\n",
    "    assert log_propensities.shape == target_propensities.shape\n",
    "    assert log_propensities.shape == (context_train.shape[0], len(all_actions))\n",
    "\n",
    "    \n",
    "    #Fit a model for each action\n",
    "    for action in all_actions:\n",
    "        context_for_action = context_train[actions_train==action,:]\n",
    "        rewards_for_action = rewards_train[actions_train==action]\n",
    "        model = RidgeCV()\n",
    "        t_prop_action = target_propensities[actions_train==action,action]\n",
    "        l_prop_action = log_propensities[actions_train==action,action]\n",
    "        importance_weights = np.divide(t_prop_action, l_prop_action, out=np.zeros_like(t_prop_action), where=l_prop_action!=0)\n",
    "        model.fit(context_for_action,rewards_for_action,sample_weight=importance_weights )\n",
    "        action_to_model_dict[action] = model\n",
    "    # Models are fit\n",
    "    return action_to_model_dict\n",
    "\n",
    "def evaluate(context_test,map_picks_test,actions_test,rewards_test,log_policy,target_bandit,action_to_model_dict):\n",
    "    est = {}\n",
    "    est[\"mean\"] = np.mean(rewards_test)\n",
    "    \n",
    "    all_actions = action_to_model_dict.keys()\n",
    "    num_actions = target_bandit.n_arms\n",
    "    assert target_bandit.n_arms == len(log_policy.pa_x_dict[6])\n",
    "    \n",
    "    #Create Logging policies propensity distribution\n",
    "    log_propensities = np.empty((context_test.shape[0],len(log_policy.map_cols)))\n",
    "    for ii,(idx,row) in enumerate(map_picks_test.iterrows()):  \n",
    "        log_propensities_row = log_policy.predict_proba(row)\n",
    "        log_propensities[ii,:] = log_propensities_row\n",
    "    \n",
    "    #Create target policies propensity distribution\n",
    "    target_propensities = np.empty((context_test.shape[0],num_actions))\n",
    "    for ii in range(context_test.shape[0]):   \n",
    "        target_propensities_row = target_bandit.predict_proba(context_test[ii,:])\n",
    "        target_propensities[ii,:] = target_propensities_row\n",
    "   \n",
    "    #( Self-normalized) Importance weighted value estimator\n",
    "    \n",
    "    importance_weights_matrix = np.divide(target_propensities,log_propensities,out=np.zeros_like(target_propensities), where=log_propensities!=0)\n",
    "    importance_weights = np.choose(actions_test,importance_weights_matrix.T)\n",
    "\n",
    "    est['IW'] = (rewards_test * importance_weights).mean()\n",
    "    \n",
    "    #SN IW estimator\n",
    "    est['SN_IW'] = (rewards_test*importance_weights).sum()/importance_weights.sum()\n",
    "    \n",
    "    #Direct Method\n",
    "    \n",
    "    predicted_rewards = np.empty((context_test.shape[0],num_actions))\n",
    "    #Create predicted reward distribution\n",
    "    for action in all_actions:\n",
    "        model = action_to_model_dict[action]\n",
    "        predicted_rewards[:,action] = model.predict(context_test)\n",
    "    # estimate\n",
    "    est['Direct_Method_IW'] = (predicted_rewards*target_propensities).sum(axis=1).mean()\n",
    "    return est\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_model_dict = train_value_estimator(X_train,map_pick_context_train, A_train, Y_train, log_policy=lp, target_bandit=bandit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform policy: untrained bandit\n",
    "untrained_bandit = Bandit(X_train.shape[1], n_arms, step_size=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MHS: Nothing important different down here, except I added the Test data section too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Bandit:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': 0.5495714285714286,\n",
       " 'IW': 1.2872922442626826,\n",
       " 'SN_IW': 0.5567765383510583,\n",
       " 'Direct_Method_IW': 0.5571072945472595}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Trained Bandit:\")\n",
    "evaluate(X_train, map_pick_context_train, A_train, Y_train, \\\n",
    "         log_policy=lp, target_bandit=bandit, action_to_model_dict=action_to_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform Policy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': 0.5495714285714286,\n",
       " 'IW': 1.254442289045975,\n",
       " 'SN_IW': 0.5558585769907851,\n",
       " 'Direct_Method_IW': 0.5557734225238844}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Uniform Policy:\")\n",
    "evaluate(X_train, map_pick_context_train, A_train, Y_train, \\\n",
    "         log_policy=lp, target_bandit=untrained_bandit,action_to_model_dict=action_to_model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Bandit:\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': 0.5362649914334666,\n",
       " 'IW': 1.1683165867327745,\n",
       " 'SN_IW': 0.5673088334865963,\n",
       " 'Direct_Method_IW': 0.558290557623652}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Trained Bandit:\")\n",
    "evaluate(X_test, map_pick_context_test, A_test, Y_test, \\\n",
    "         log_policy=lp, target_bandit=bandit, action_to_model_dict=action_to_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform Policy:\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 549 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n",
      "Team ID 18 not seen during training. Using default policy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': 0.5362649914334666,\n",
       " 'IW': 1.155900524260706,\n",
       " 'SN_IW': 0.5674112357209264,\n",
       " 'Direct_Method_IW': 0.5568874670342726}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Uniform Policy:\")\n",
    "evaluate(X_test, map_pick_context_test, A_test, Y_test, \\\n",
    "         log_policy=lp, target_bandit=untrained_bandit, action_to_model_dict=action_to_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
