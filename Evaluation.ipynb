{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "historical-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandit import Bandit\n",
    "from context_engineering_functions import *\n",
    "from logging_policy import LoggingPolicy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-repository",
   "metadata": {},
   "source": [
    "Need to take a Bandit object - which contains a policy to evaluate, and also a Logging Policy Object.\n",
    "We estimate the value using the Direct Method, where we fit a ridge regression w/ importance weights onto the reward function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "optimum-stopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Basic Context Engineering\n"
     ]
    }
   ],
   "source": [
    "data_directory = './csgo_clean/'\n",
    "map_pick_context = create_basic_triples(data_directory)\n",
    "map_pick_context_train, map_pick_context_test  = train_test_split(map_pick_context,test_size=.2,train_size=.8,shuffle=False)\n",
    "\n",
    "cols = [col for col in map_pick_context if col.endswith('is_available')]\n",
    "X_train = map_pick_context_train[cols].values\n",
    "A_train = map_pick_context_train['X_Action'].values\n",
    "Y_train = map_pick_context_train['Y_reward'].values\n",
    "\n",
    "X_test = map_pick_context_test[cols].values\n",
    "A_test = map_pick_context_test['X_Action'].values\n",
    "Y_test = map_pick_context_test['Y_reward'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sticky-steps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for random row: \n",
      "[[0.2 0.2 0.  0.2 0.2 0.2 0. ]]\n"
     ]
    }
   ],
   "source": [
    "n_arms = 7\n",
    "\n",
    "# randomly chosen by dice roll\n",
    "n = 4569\n",
    "# n = np.random.choice(context.index)\n",
    "\n",
    "bandit = Bandit(X_train.shape[1], n_arms, step_size=0.01)\n",
    "\n",
    "print('Probabilities for random row: ')\n",
    "print(bandit.predict_proba(X_train[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "center-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = LoggingPolicy(map_pick_context_train,map_pick_context_train['X_Action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "operational-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    bandit.update_theta(X_train[i].reshape(1, -1), A_train[i], Y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-deviation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "confirmed-warren",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for random row after training: \n",
      "[[0.16526486 0.2547316  0.         0.26307424 0.1534473  0.163482\n",
      "  0.        ]]\n",
      "Selected action: \n",
      "[3]\n",
      "Actual action and reward for random row: \n",
      "Action: 4, reward: 0\n"
     ]
    }
   ],
   "source": [
    "print('Probabilities for random row after training: ')\n",
    "print(bandit.predict_proba(X_train[n]))\n",
    "print('Selected action: ')\n",
    "print(bandit.predict(X_train[n]))\n",
    "\n",
    "print('Actual action and reward for random row: ')\n",
    "print(f'Action: {A_train[n]}, reward: {Y_train[n]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-evanescence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "funded-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_value_estimator(context_train,map_picks_train,actions_train,rewards_train,log_policy,target_bandit):\n",
    "    '''\n",
    "    Trains an importance weighted RidgeCV model which is used for direct method estimation.\n",
    "    \n",
    "    Input:\n",
    "        context_train (np.array(n x k)): n is number of train examples. k is dimensionality of context.\n",
    "        map_picks_train (df)\n",
    "            map_picks train and context train must be created using this line of code:\n",
    "            \"context_train = map_pick_context_train[cols].values\"\n",
    "\n",
    "        actions_train (np.array(n)) : actions taken\n",
    "        rewards (np.array(n): rewards received\n",
    "        log_policy: a LoggingPolicy object, which needs to have a function predict_proba(self,context)\n",
    "        target_policy: a Bandit object, which needs to have a function predict_proba(self,context)\n",
    "    '''\n",
    "    all_actions = np.unique(actions_train) # return from unique is already sorted\n",
    "    action_to_model_dict = {}\n",
    "    log_propensities = np.empty((context_train.shape[0],len(log_policy.map_cols)))\n",
    "    \n",
    "    for ii,(idx,row) in enumerate(map_picks_train.iterrows()):   \n",
    "        log_propensities_row = log_policy.predict_proba(row)\n",
    "        log_propensities[ii,:] = log_propensities_row\n",
    "    \n",
    "    target_propensities = np.empty((context_train.shape[0],len(log_policy.map_cols)))\n",
    "    \n",
    "    for ii in range(X_train.shape[0]):   \n",
    "        target_propensities_row = target_bandit.predict_proba(X_train[ii,:])\n",
    "        target_propensities[ii,:] = target_propensities_row\n",
    "    \n",
    "    # Check to make sure these are both n x k. I guess its possible that not all actions were chosen, but thats unlikely.\n",
    "    assert log_propensities.shape == target_propensities.shape\n",
    "    assert log_propensities.shape == (context_train.shape[0], len(all_actions))\n",
    "\n",
    "    \n",
    "    #Fit a model for each action\n",
    "    for action in all_actions:\n",
    "        context_for_action = context_train[actions_train==action,:]\n",
    "        rewards_for_action = rewards_train[actions_train==action]\n",
    "        model = RidgeCV()\n",
    "        t_prop_action = target_propensities[actions_train==action,action]\n",
    "        l_prop_action = log_propensities[actions_train==action,action]\n",
    "        importance_weights = np.divide(t_prop_action, l_prop_action, out=np.zeros_like(t_prop_action), where=l_prop_action!=0)\n",
    "        model.fit(context_for_action,rewards_for_action,sample_weight=importance_weights )\n",
    "        action_to_model_dict[action] = model\n",
    "    # Models are fit\n",
    "    return action_to_model_dict\n",
    "\n",
    "def evaluate(context_test,map_picks_test,actions_test,rewards_test,log_policy,target_bandit,action_to_model_dict):\n",
    "    est = {}\n",
    "    est[\"mean\"] = np.mean(rewards_test)\n",
    "    \n",
    "    all_actions = action_to_model_dict.keys()\n",
    "    num_actions = target_bandit.n_arms\n",
    "    assert target_bandit.n_arms == len(log_policy.full_action.unique())\n",
    "    \n",
    "    #Create Logging policies propensity distribution\n",
    "    log_propensities = np.empty((context_test.shape[0],len(log_policy.map_cols)))\n",
    "    for ii,(idx,row) in enumerate(map_picks_test.iterrows()):  \n",
    "        log_propensities_row = log_policy.predict_proba(row)\n",
    "        log_propensities[ii,:] = log_propensities_row\n",
    "    \n",
    "    #Create target policies propensity distribution\n",
    "    target_propensities = np.empty((context_test.shape[0],num_actions))\n",
    "    for ii in range(X_train.shape[0]):   \n",
    "        target_propensities_row = target_bandit.predict_proba(X_train[ii,:])\n",
    "        target_propensities[ii,:] = target_propensities_row\n",
    "   \n",
    "    #( Self-normalized) Importance weighted value estimator\n",
    "    \n",
    "    importance_weights_matrix = np.divide(target_propensities,log_propensities,out=np.zeros_like(target_propensities), where=log_propensities!=0)\n",
    "    importance_weights = np.choose(actions_test,importance_weights_matrix.T)\n",
    "\n",
    "    est['IW'] = (rewards_test * importance_weights).mean()\n",
    "    \n",
    "    #SN IW estimator\n",
    "    est['SN_IW'] = (rewards_test*importance_weights).sum()/importance_weights.sum()\n",
    "    \n",
    "    #Direct Method\n",
    "    \n",
    "    predicted_rewards = np.empty((context_test.shape[0],num_actions))\n",
    "    #Create predicted reward distribution\n",
    "    for action in all_actions:\n",
    "        model = action_to_model_dict[action]\n",
    "        predicted_rewards[:,action] = model.predict(context_test)\n",
    "    # estimate\n",
    "    est['Direct_Method_IW'] = (predicted_rewards*target_propensities).sum(axis=1).mean()\n",
    "    return est\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-township",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "phantom-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_model_dict = train_value_estimator(X_train,map_pick_context_train, A_train, Y_train, log_policy=lp, target_bandit=bandit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "modern-constraint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.5495714285714286,\n",
       " 'IW': 1.2872922442626826,\n",
       " 'SN_IW': 0.5567765383510582,\n",
       " 'Direct_Method_IW': 0.5571072945472592}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(X_train, map_pick_context_train, A_train, Y_train, log_policy=lp, target_bandit=bandit, action_to_model_dict=action_to_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ready-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_bandit = Bandit(X_train.shape[1], n_arms, step_size=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "going-laundry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.5495714285714286,\n",
       " 'IW': 1.254442289045975,\n",
       " 'SN_IW': 0.5558585769907851,\n",
       " 'Direct_Method_IW': 0.5557734225238841}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(X_train, map_pick_context_train, A_train, Y_train,log_policy=lp, target_bandit=untrained_bandit,action_to_model_dict=action_to_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-service",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
